---
permalink: /
title: 
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a postdoctoral researcher in the [Department of Computer Science](https://www.cs.princeton.edu/) at Princeton University, advised by [Tom Griffiths](https://cocosci.princeton.edu/tom/index.php). Starting in January 2024, I will be an Assistant Professor in the [Department of Linguistics](https://ling.yale.edu/) at Yale University. Before coming to Princeton, I received a Ph.D. in Cognitive Science at Johns Hopkins University, co-advised by [Tal Linzen](http://tallinzen.net/) and [Paul Smolensky](http://cogsci.jhu.edu/directory/paul-smolensky/), and before that I received a B.A. in Linguistics at Yale University, advised by [Robert Frank](https://ling.yale.edu/people/robert-frank).

I study computational linguistics using techniques from cognitive science, machine learning, and natural language processing. My research focuses on how to achieve **robust generalization** in models of language, as this remains one of the main areas where current AI systems fall short and one of the most impressive components of language processing in humans. In particular, I study which **inductive biases** and which **representations of structure** enable robust generalization, since these are two of the major components that determine how learners generalize to novel types of input.

For a more detailed overview of my research interests, you can read [this summary for a linguistics/cognitive science audience](https://rtmccoy.com/topics/files/mccoy_ling_research_statement_10sept2023.pdf) or [this summary for a computer science audience](mccoy_nlp_research_statement_10sept2023.pdf). 

**Prospective PhD students:** I will be accepting PhD students for Fall 2024, through either the Yale Linguistics Department or the Yale Computer Science Department.

**Conversation topics:** Do you have a meeting scheduled with me but don't know what to talk about? See [this page](https://rtmccoy.com/topics/) for some topics that are often on my mind.

**Representative papers:**
- <b>R. Thomas McCoy</b>, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. <em>arXiv preprint arXiv 2111.09509</em>. [<a href="https://arxiv.org/pdf/2111.09509.pdf">pdf</a>]
- **R. Thomas McCoy**, Jennifer Culbertson, Paul Smolensky, and GÃ©raldine Legendre. Infinite use of finite means? Evaluating the generalization of center embedding learned from an artificial grammar. <em>Proceedings of the 43rd Annual Conference of the Cognitive Science Society</em>. [<a href="https://psyarxiv.com/r8ct2">pdf</a>] 
- **R. Thomas McCoy**, Erin Grant, Paul Smolensky, Thomas L. Griffiths, and Tal Linzen. Universal linguistic inductive biases via meta-learning. *Proceedings of the 42nd Annual Conference of the Cognitive Science Society*. [<a href="https://arxiv.org/pdf/2006.16324.pdf">pdf</a>] [<a href="http://rtmccoy.com/meta-learning-linguistic-biases.html">demo</a>]
- **R. Thomas McCoy**, Ellie Pavlick, and Tal Linzen. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. *ACL 2019*. [<a href="https://www.aclweb.org/anthology/P19-1334.pdf">pdf</a>]  
- **R. Thomas McCoy**, Robert Frank, and Tal Linzen. Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks. *TACL*. [<a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00304">pdf</a>] [<a href="http://rtmccoy.com/rnn_hierarchical_biases.html">website</a>]
- **R. Thomas McCoy**, Tal Linzen, Ewan Dunbar, and Paul Smolensky. RNNs implicitly implement Tensor Product Representations. *ICLR 2019*. [<a href="https://openreview.net/pdf?id=BJx0sjC5FX">pdf</a>] [<a href="https://tommccoy1.github.io/tpdn/tpr_demo.html">demo</a>]


