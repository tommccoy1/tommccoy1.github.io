---
permalink: /pubs/
author_profile: true
redirect_from:
  - /pubs
---

{% include base_path %}

<em>This page is only updated sporadically. For a fully up-to-date list, see <a href="https://scholar.google.com/citations?user=xSavR6cAAAAJ&hl=en">my Google Scholar page</a>.</em>


# 2025

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b> and Thomas L. Griffiths. Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. <em>Nature Communications</em>. [<a href="https://www.nature.com/articles/s41467-025-59957-y">paper</a>]
</p>


<p style="margin-left: 40px; text-indent: -40px;">
Tong Zheng, Lichang Chen, Simeng Han, <b>R. Thomas McCoy</b>, and Heng Huang. Learning to Reason via Mixture-of-Thought for Logical Reasoning. <em>arXiv preprint arXiv 2505.15817</em>. [<a href="https://arxiv.org/pdf/2505.15817">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Simeng Han, Stephen Xia, Grant Zhang, Howard Dai, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, and <b>R. Thomas McCoy</b>. Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models. <em>arXiv preprint arXiv 2505.10844</em>. [<a href="https://arxiv.org/pdf/2505.10844">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Nathan A. Chi, Teodor Malchev, Riley Kong, Ryan A. Chi, Lucas Huang, Ethan A. Chi, <b>R. Thomas McCoy</b>, and Dragomir Radev. ModeLing: A novel dataset for testing linguistic reasoning in language models. <em>Proceedings of the Eighth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2025)</em>. [<a href="https://aclanthology.org/2025.loresmt-1.10.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Liyi Zhang, Veniamin Veselovsky, <b>R. Thomas McCoy</b>, and Thomas L. Griffiths. Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models. <em>arXiv preprint arXiv 2504.12585</em>. [<a href="https://arxiv.org/pdf/2504.12585">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Max Gupta, Sunayana Rane, <b>R. Thomas McCoy</b>, and Thomas L. Griffiths. Convolutional Neural Networks Can (Meta-)Learn the Same-Different Relation.  <em>Proceedings of the 47th Annual Meeting of the Cognitive Science Society</em>. [<a href="https://arxiv.org/pdf/2503.23212">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Ioana Marinescu, <b>R. Thomas McCoy</b>, and Thomas L. Griffiths. Neural Networks Can Capture Human Concept Learning Without Assuming Symbolic Representations. <em>OSF preprint</em>. [<a href="https://osf.io/preprints/psyarxiv/3qjfg_v1">OSF link</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Alexander Ku, Declan Campbell, Xuechunzi Bai, Jiayi Geng, Ryan Liu, Raja Marjieh, <b>R. Thomas McCoy</b>, Andrew Nam, Ilia Sucholutsky, Veniamin Veselovsky, Liyi Zhang, Jian-Qiao Zhu, and Thomas L. Griffiths. Using the tools of cognitive science to understand large language models at different levels of analysis. <em>arXiv preprint arXiv 2503.13401</em>. [<a href="https://arxiv.org/pdf/2503.13401">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Gianluca Bencomo, Max Gupta, Ioana Marinescu, <b>R. Thomas McCoy</b>, and Thomas L. Griffiths. Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks. <em>Proceedings of the 47th Annual Meeting of the Cognitive Science Society</em>. [<a href="https://arxiv.org/pdf/2502.20237">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Zhenghao Zhou, Robert Frank, <b>R. Thomas McCoy</b>. Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming. <em>NAACL</em>. [<a href="https://aclanthology.org/2025.naacl-long.586.pdf">pdf</a>]
</p>


# 2024

<p style="margin-left: 40px; text-indent: -40px;">
Leroy Z. Wang, <b>R. Thomas McCoy</b>, and Shane Steinert-Threlkeld. Minimization of Boolean Complexity in In-Context Concept Learning. <em>arXiv preprint arXiv 2412.02823</em>. [<a href="https://arxiv.org/pdf/2412.02823">pdf</a>]
</p>


<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of autoregression show how large language models are shaped by the problem they are trained to solve. <em>Proceedings of the National Academy of Sciences</em>. [<a href="https://www.pnas.org/doi/pdf/10.1073/pnas.2322420121">pdf</a>]
</p>



<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Shunyu Yao, Dan Friedman, Mathew D. Hardy, and Thomas L. Griffiths. When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1. <em>arXiv preprint arXiv 2410.01792</em>. [<a href="https://arxiv.org/pdf/2410.01792">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Thomas L. Griffiths, Jian-Qiao Zhu, Erin Grant, and <b>R. Thomas McCoy</b>. Bayes in the Age of Intelligent Machines. <em>Current Directions in Psychological Science</em>. [<a href="https://journals.sagepub.com/doi/pdf/10.1177/09637214241262329?casa_token=BkkOwaVJKPAAAAAA:Ddb7oMAFas6_UWvqx9b9wngYgzlfMwKztxqCb8x2f01rUKzJ29GGhNejxPVmB4xtDcu-fkSWLjNMLg">pdf</a>]
</p>


<p style="margin-left: 40px; text-indent: -40px;">
Akshara Prabhakar, Thomas L. Griffiths, and <b>R. Thomas McCoy</b>. Deciphering the factors influencing the efficacy of chain-of-thought: Probability, memorization, and noisy reasoning. <em>Findings of EMNLP.</em> [<a href="https://aclanthology.org/2024.findings-emnlp.212.pdf">pdf</a>]
</p>


<p style="margin-left: 40px; text-indent: -40px;">
Ioana Marinescu, <b>R. Thomas McCoy</b>, and Thomas L. Griffiths. Distilling symbolic priors for concept learning into neural networks. <em>Proceedings of the 46th Annual Meeting of the Cognitive Science Society.</em> [<a href="https://escholarship.org/uc/item/2rz450nc">CogSci link</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b> and Thomas L. Griffiths. Meta-learning as a bridge between neural networks and symbolic Bayesian models. <em>Behavioral and Brain Sciences</em> (response to "Meta-learned models of cognition"). [<a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/metalearning-as-a-bridge-between-neural-networks-and-symbolic-bayesian-models/185DB00366FD4F9B218E36F32886242F">BBS link</a>]
</p>


# 2023

<p style="margin-left: 40px; text-indent: -40px;">
Liyi Zhang, <b>R. Thomas McCoy</b>, Theodore R Sumers, Jian-Qiao Zhu, Thomas L. Griffiths. Deep de finetti: Recovering topic distributions from large language models. <em>arXiv preprint arXiv 2312.14226</em>. [<a href="https://arxiv.org/pdf/2312.14226">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. <em>Transactions of the Association for Computational Linguistics</em>. [<a href="https://aclanthology.org/2023.tacl-1.38.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Aditya Yedetore, Tal Linzen, Robert Frank, and <b>R. Thomas McCoy</b>. How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em>. [<a href="https://aclanthology.org/2023.acl-long.521.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Thomas L. Griffiths, Sreejan Kumar, and <b>R. Thomas McCoy</b>. On the hazards of relating representations and inductive biases. <em>Behavioral and Brain Sciences</em> (response to "The best game in town: The reemergence of the language-of-thought hypothesis across the cognitive sciences"). [<a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/on-the-hazards-of-relating-representations-and-inductive-biases/05A247FA8325343E035E73EC92DE88D1">BBS link</a>]
</p>


# 2022
<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>. Implicit compositional structure in the vector representations of artificial neural networks. Johns Hopkins University PhD dissertation. [<a href="https://jscholarship.library.jhu.edu/server/api/core/bitstreams/3cb3efd5-f7f8-4e54-a74e-db0802d72f9b/content">pdf</a>] 
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Paul Smolensky, <b>R. Thomas McCoy</b>, Roland Fernandez, Matthew Goldrick, and Jianfeng Gao. Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems. <em>AI Magazine</em>. [<a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/18599">AI Magazine link</a>] 
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Paul Smolensky, <b>R. Thomas McCoy</b>, Roland Fernandez, Matthew Goldrick, and Jianfeng Gao. Neurocompositional computing in human and machine intelligence: A tutorial. <em>Microsoft technical report</em>. [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2022/04/Neurocompositional_computing__tutorial.pdf">odf</a>] 
</p>



# 2021

<p style="margin-left: 40px; text-indent: -40px;">
Paul Soulos, Sudha Rao, Caitlin Smith, Eric Rosen, Asli Celikyilmaz, <b>R. Thomas McCoy</b>, Yichen Jiang, Coleman Haley, Roland Fernandez, Hamid Palangi, Jianfeng Gao, and Paul Smolensky. Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages. <em>Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages</em>.
</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Jennifer Culbertson, Paul Smolensky, and GÃ©raldine Legendre. Infinite use of finite means? Evaluating the generalization of center embedding learned from an artificial grammar. <em>Proceedings of the 43rd Annual Conference of the Cognitive Science Society</em>. [<a href="https://psyarxiv.com/r8ct2">pdf</a>]
</p>



# 2020

<p style="margin-left: 40px; text-indent: -40px;">
Michael Lepori and <b>R. Thomas McCoy</b>. Picking BERT's Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis. <em>Proceedings of the 28th International Conference on Computational Linguistics (COLING)</em>. [<a href="https://aclanthology.org/2020.coling-main.325/">pdf</a>]
</p>


<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Junghyun Min, and Tal Linzen. BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. <em>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</em>. [<a href="https://aclanthology.org/2020.blackboxnlp-1.21.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Paul Soulos, <b>R. Thomas McCoy</b>, Tal Linzen, and Paul Smolensky. Discovering the compositional structure of vector representations with Role Learning Networks. In <em>Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</em>. [<a href="https://aclanthology.org/2020.blackboxnlp-1.23.pdf">pdf</a>]
</p>


<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Erin Grant, Paul Smolensky, Thomas L. Griffiths, and Tal Linzen. Universal linguistic inductive biases via meta-learning. <em>Proceedings of the 42nd Annual Conference of the Cognitive Science Society</em>. [<a href="https://arxiv.org/pdf/2006.16324.pdf">pdf</a>] [<a href="http://rtmccoy.com/meta-learning-linguistic-biases.html">demo</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Michael Lepori, Tal Linzen, and <b>R. Thomas McCoy</b>. Representations of syntax [MASK] useful: Effects of constituency and dependency structure in recursive LSTMs. <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. [<a href="https://arxiv.org/pdf/2005.00019">pdf</a>]
</p> 


<p style="margin-left: 40px; text-indent: -40px;">
Junghyun Min, <b>R. Thomas McCoy</b>, Dipanjan Das, Emily Pitler, and Tal Linzen. Syntactic data augmentation increases robustness to inference heuristics. <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. [<a href="https://arxiv.org/pdf/2004.11999">pdf</a>]
</p> 

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Robert Frank, and Tal Linzen. Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks. <em>Transactions of the Association for Computational Linguistics</em>. [<a href="https://arxiv.org/pdf/2001.03632.pdf">arXiv</a>] [<a href="http://rtmccoy.com/rnn_hierarchical_biases.html">website</a>]
</p>

# 2019

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Ellie Pavlick, and Tal Linzen. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>. [<a href="https://www.aclweb.org/anthology/P19-1334/">ACL anthology</a>] [<a href="https://www.aclweb.org/anthology/P19-1334.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Samuel R. Bowman, Ellie Pavlick, Edouard Grave, Benjamin Van Durme, Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, <b>R. Thomas McCoy</b>, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, and Berlin Chen. Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling. In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>. [<a href="https://www.aclweb.org/anthology/P19-1439/">ACL anthology</a>] [<a href="https://www.aclweb.org/anthology/P19-1439.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>. Touch <em>down</em> in Pittsburghese. In <em>Yale Working Papers in Grammatical Diversity</em>. [<a href="https://elischolar.library.yale.edu/cgi/viewcontent.cgi?article=1002&context=ygdp">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Najoung Kim, Roma Patel, Adam Poliak, Alex Wang, Patrick Xia, <b>R. Thomas McCoy</b>, Ian Tenney, Alexis Ross, Tal Linzen, Benjamin Van Durme, Samuel R. Bowman, and Ellie Pavlick. Probing What Different NLP Tasks Teach Machines about Function Word Comprehension. In <em>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (&ast;SEM 2019)</em>. [<a href="https://www.aclweb.org/anthology/S19-1026/">ACL anthology</a>] [<a href="https://www.aclweb.org/anthology/S19-1026v2.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Tal Linzen, Ewan Dunbar, and Paul Smolensky. RNNs implicitly implement tensor-product representations. In <em>International Conference on Learning Representations 2019</em>. [<a href="https://openreview.net/forum?id=BJx0sjC5FX">OpenReview</a>] [<a href="https://openreview.net/pdf?id=BJx0sjC5FX">pdf</a>] [<a href="https://tommccoy1.github.io/tpdn/tpr_demo.html">demo</a>] [<a href="https://tommccoy1.github.io/files/iclr_handout.pdf">poster</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, <b>R. Thomas McCoy</b>, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? Probing for sentence structure in contextualized word representations. In <em>International Conference on Learning Representations 2019</em>. [<a href="https://openreview.net/forum?id=SJzSgnRcKX">OpenReview</a>] [<a href="https://openreview.net/pdf?id=SJzSgnRcKX">pdf</a>]

</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b> and Tal Linzen. Non-entailed subsequences as a challenge for natural language inference. In <em>Proceedings of the Society for Computation in Linguistics (SCiL) 2019</em>. [<a href="https://arxiv.org/abs/1811.12112">arxiv</a>] [<a href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1073&context=scil">pdf</a>]

</p>

# 2018

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>, Robert Frank, and Tal Linzen. 2018. Revisiting the poverty of
the stimulus: hierarchical generalization without a hierarchical bias in recurrent
neural networks. In <em>Proceedings of the 40th Annual Conference of the
Cognitive Science Society</em>. [<a href="https://arxiv.org/abs/1802.09091">arxiv</a>] [<a href="https://arxiv.org/pdf/1802.09091.pdf">pdf</a>] 
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Patrick Littell, <b>R. Thomas McCoy</b>, Na-Rae Han, Shruti Rijhwani, Zaid Sheikh,
David Mortensen, Teruko Mitamura, and Lori Levin. 2018. Parser combinators for
Tigrinya and Oromo morphology. In <em>Language Resources and Evaluation
Conference (LREC) 2018</em>. [<a href="https://www.aclweb.org/anthology/L18-1611">ACL anthology</a>] [<a href="https://www.aclweb.org/anthology/L18-1611.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b> and Robert Frank. 2018. Phonologically Informed Edit Distance
Algorithms for Word Alignment with Low-Resource Languages. In <em>Proceedings
of the Society for Computation in Linguistics (SCiL) 2018</em>, pages 102-112. [<a href="https://www.aclweb.org/anthology/W18-0311">ACL anthology</a>] [<a href="https://www.aclweb.org/anthology/W18-0311.pdf">pdf</a>]

</p>

# 2017

<p style="margin-left: 40px; text-indent: -40px;">
Jungo Kasai, Bob Frank, <b>R. Thomas McCoy</b>, Owen Rambow, and Alexis Nasr. 2017.
Tag parsing with neural networks and vector representations of supertags. In
<em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing</em>, pages 1712-1722. [<a href="https://www.aclweb.org/anthology/D17-1180">ACL anthology</a>] [<a href="https://www.aclweb.org/anthology/D17-1180.pdf">pdf</a>]
</p>

<p style="margin-left: 40px; text-indent: -40px;">
Dan Friedman&ast;, Jungo Kasai&ast;, <b>R. Thomas McCoy</b>&ast;, Robert Frank, Forrest Davis,
Owen Rambow. Linguistically Rich Vector Representations of Supertags for TAG
Parsing. In <em>Proceedings of the 13th International Workshop on Tree Adjoining
Grammars and Related Formalisms</em>, pages 122-131. [<a href="https://www.aclweb.org/anthology/W17-6213">ACL anthology</a>] [<a href="https://www.aclweb.org/anthology/W17-6213.pdf">pdf</a>]
&ast;Equal contribution.
</p>

<p style="margin-left: 40px; text-indent: -40px;">
<b>R. Thomas McCoy</b>. 2017. English comparatives as degree-phrase relative clauses. In
<em>Proceedings of the Linguistic Society of America 2</em>, 26:1-7. [<a href="https://journals.linguisticsociety.org/proceedings/index.php/PLSA/article/view/4078">link</a>]
</p>
